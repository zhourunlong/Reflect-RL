import json
import os
import random
import time

from autogen import AssistantAgent, UserProxyAgent

from nat_lang_envs.base_env import NaturalLanguageEnvironment
from nat_lang_envs.taxi import ACTION_DICT, TaxiNatLangEnv

MODEL_TYPE = "OpenAI"
MODEL = "gpt-4"

# MODE = "pickup"
MODE = "full"

TRAJ_ALGO = "oracle + random 0.9"    # one of ["oracle", "oracle + random x", "gpt"]
GEN_REFLECTION = True
REFLECT_WITH_SPOILER = True

if MODEL_TYPE == "OpenAI":
    config_list = [
        {
            "model": MODEL,
            "api_key": os.environ["OPENAI_API_KEY"],
        }
    ]
elif MODEL_TYPE == "AzureOpenAI":
    config_list = [
        {
            "model": MODEL,
            "api_key": os.environ["AZURE_OPENAI_KEY"],
            "base_url": os.environ["AZURE_BASE_URL"].replace("/v1", "")
        }
    ]

os.makedirs("data/taxi/", exist_ok=True)
if GEN_REFLECTION:
    OUT_FILE = "data/taxi/data_reflection.json"
else:
    OUT_FILE = "data/taxi/data.json"

if TRAJ_ALGO == "oracle":
    OUT_FILE = OUT_FILE.replace(".json", "_oracle.json")

if os.path.exists(OUT_FILE):
    DATA = json.load(open(OUT_FILE, "r"))
else:
    DATA = []

llm_config = {"config_list": config_list, "cache_seed": 42}

###

DONE_PROBLEMS = [_.get("problem", "") for _ in DATA]
DONE_PROBLEMS = set(DONE_PROBLEMS)
HORIZON = 15

# For GPT models to generate reason and action
planner = None

# For non-GPT models to generate reason
reasoner = AssistantAgent(
    name="reasoner",
    system_message="""Given a problem state and a selected action (from you).
You need to give reason why you take this action.
Remember to give a reason that is consistent with the action, within 100 words.
Even if you feel like the action is wrong (in hindsight),
you still decided to take this action, and explain why.


For instance,
Because ..., so I ...
The task is to,... I
I found ... So...
etc.
""",
    llm_config=llm_config,
    max_consecutive_auto_reply=1,
    code_execution_config={"use_docker": False})


if TRAJ_ALGO == "oracle":
    # extra_info = "Note that the previous actions are ALMOST perfect, because most of them are generated by the oracle."
    extra_info = "Note that the previous actions are PERFECT, because they are generated by the oracle."
else:
    extra_info = "The previous actions might contain some mistakes."

reflectioner = AssistantAgent(
    name="reflectioner",
    system_message=
    """Given a problem state, the actions you have taken, and the observations you have.

You need to give reflection on your actions, such as:
- What is the consequence of your previous action?
- How is your previous action? Good or bad? Why?
- What is the next action you want to take if possible? Why?

I might give you some spoiler information and optimal action for cheating, but you should not mention that you have seen any spoilers, optimal actions, or any other information that you should not know.
Pretend you are smart and just know these information.

Don't use any words related to "optimal" in your reflection.

Keep your reflection concise within 100 words.

For instance,
Because ..., so I ...
The task is to,... I
I found ... So...
etc.
""",
    llm_config=llm_config,
    max_consecutive_auto_reply=1,
    code_execution_config={"use_docker": False})

user = UserProxyAgent(name="user", max_consecutive_auto_reply=1,
                      code_execution_config={"use_docker": False})

REFLECTION_PROMPT = "------ State ------\n{state}\n\n------ Give me a two-sentence reflection ---"
REFLECTION_PROMPT_WITH_SPOILER = "------ State ------\n{state}\n\n--- Spoiler: the best action you should take next is {next_action} ---\n\n------ Give me a two-sentence reflection ---"

FIRST_STEP_REFLECTION_PROMPT = "------ State ------\n{state}\n\n------ Give me a one-sentence analysis on what I should do  ---"
FIRST_STEP_REFLECTION_PROMPT_WITH_SPOILER  = "------ State ------\n{state}\n\n--- Spoiler: the best action you should take next is {next_action} ---\n\n------ Give me a one-sentence analysis on what I should do  ---"

def create_env():
    env = TaxiNatLangEnv(with_prompt=True,
                         horizon=20,
                         with_history_in_prompt=True)
    return env

def user_send(user, reason_prompt, reasoner):
    user.send(message=reason_prompt,
              recipient=reasoner,
              request_reply=True,
              silent=False)
    return user

def env_step(env, action):
    return env.step(action)


while True:
    try:
        env = create_env()
        env.reset(data=MODE)
    except Exception as e:
        print(e)
        env = None

    if env is None or not isinstance(env, NaturalLanguageEnvironment) or env.env is None:
        print("Env error. the env is:", env)
        continue

    done = False
    steps = 0

    prompt_batch = []

    total_reward = 0
    are_prev_actions_oracle = []
    while not done:
        # Option 1: use oracle
        if TRAJ_ALGO == "oracle":
            action = env.oracle_action[0]
            are_prev_actions_oracle.append(True)
        # Option 2: use random
        elif TRAJ_ALGO.find("oracle + random") >= 0:
            prob = float(TRAJ_ALGO.split("oracle + random ")[-1])
            if random.random() > prob:
                action = env.oracle_action[0]
                are_prev_actions_oracle.append(True)
            else:
                action = random.choice(env.action_space)
                are_prev_actions_oracle.append(False)
        # Option 3: Let GPT decide
        elif TRAJ_ALGO == "gpt":
            # Planner needs to perform ReAct
            action = planner.act(env.prompt, env.action_space)
            are_prev_actions_oracle.append(False)
        elif TRAJ_ALGO == "random":
            # action = random.choice([0, 1, 2, 3, 4]) # remove drop off
            action = random.choice(env.action_space)
            are_prev_actions_oracle.append(False)


        if action == "TERMINATE":
            # Some issue with the oracle path. Possibly missing Oracle path.
            break

        if GEN_REFLECTION:
            state_str = env.prompt[:env.prompt.
                                    find("The actions you can take now is")]
            oracle_action = env.oracle_action
            if len(oracle_action) > 0:
                next_action = oracle_action[0]
            else:
                next_action = None

            if next_action:
                try:
                    next_action = ACTION_DICT[int(next_action)]
                except:
                    next_action = "None, because the game ends."
            else:
                next_action = "None, because the game ends."
            if steps == 0:
                if REFLECT_WITH_SPOILER:
                    reflection_prompt =  FIRST_STEP_REFLECTION_PROMPT_WITH_SPOILER.format(
                                        state=state_str, next_action=next_action
                                        )
                else:
                    reflection_prompt = FIRST_STEP_REFLECTION_PROMPT.format(
                        state=state_str)
            elif not REFLECT_WITH_SPOILER:
                # we have observation now
                reflection_prompt = REFLECTION_PROMPT.format(
                    state=state_str)
            else:

                reflection_prompt = REFLECTION_PROMPT_WITH_SPOILER.format(
                    state=state_str, next_action=next_action
                )
            
            prompt_batch.append((env.prompt, reflection_prompt, action))


        action_result = env_step(env=env, action=action)
        steps += 1

        if are_prev_actions_oracle[-1]:
            env._obs_history[-1] = "Spoiler: this action is optimal!\n" + env._obs_history[-1]
        else:
            env._obs_history[-1] = f"Spoiler: this action may NOT be optimal, and '{next_action}' MIGHT be a better choice.\n" + env._obs_history[-1]

        # The environment crashed... Let's forget about this environment.
        if action_result is None:
            break
        state, reward, done, truncated, info = action_result
        total_reward += reward

        if truncated or done:
            break
    
    steps = len(env._action_history)
    print("Reward:", reward, "Steps:", steps)
    # if reward < 0 or steps > 20 or total_reward < 0:
    #     continue

    print(env.prompt)

    # pdb.set_trace()
    for env_prompt, reflection_prompt, action in prompt_batch:
        while len(user._oai_messages[reflectioner]) < 2:
            user.clear_history()
            try:
                rst = user_send(user=user,
                                reason_prompt=reflection_prompt,
                                reasoner=reflectioner)
                if rst is None:
                    continue
            except Exception as e:
                print(e)
                continue
            reflection = user.last_message()["content"]
            time.sleep(10) # sleep for 10 seconds because of API busy

        # print(env_prompt)
        # print(colored(reflection, "blue"))
        DATA.append({
                "state": env_prompt,
                "reflection": reflection,
                "action": action,
                "reason": "",
                "model": config_list[0].get("model", "")
        })
        json.dump(DATA, open(OUT_FILE, "w"), indent=2)

        user.clear_history()
        reasoner.clear_history()
        reflectioner.clear_history()

